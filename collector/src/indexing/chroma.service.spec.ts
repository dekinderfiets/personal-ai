import { ChromaService } from './chroma.service';
import { createHash } from 'crypto';
import { DataSource, SearchResult } from '../types';

function contentHash(text: string): string {
    return createHash('sha256').update(text).digest('hex').slice(0, 16);
}

describe('ChromaService', () => {
    let service: ChromaService;
    let mockCollection: any;
    let mockClient: any;

    beforeEach(() => {
        const mockConfigService = { get: jest.fn() } as any;
        service = new ChromaService(mockConfigService);

        mockCollection = {
            get: jest.fn().mockResolvedValue({ ids: [], documents: [], metadatas: [] }),
            query: jest.fn().mockResolvedValue({ ids: [[]], documents: [[]], metadatas: [[]], distances: [[]] }),
            upsert: jest.fn().mockResolvedValue(undefined),
            update: jest.fn().mockResolvedValue(undefined),
            delete: jest.fn().mockResolvedValue(undefined),
        };

        mockClient = {
            getOrCreateCollection: jest.fn().mockResolvedValue(mockCollection),
            deleteCollection: jest.fn().mockResolvedValue(undefined),
        };

        (service as any).client = mockClient;
        (service as any).collections = new Map();
    });

    // â”€â”€â”€ Private helper methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('chunkContent', () => {
        const chunkContent = (content: string) => (service as any).chunkContent(content);

        it('should return short content (â‰¤ 8000 chars) as a single chunk', () => {
            expect(chunkContent('hello')).toEqual(['hello']);
            expect(chunkContent('a'.repeat(8000))).toEqual(['a'.repeat(8000)]);
        });

        it('should split long content at paragraph breaks (\n\n)', () => {
            // Place \n\n at position 3500 (within the window 3200-4000)
            const content = 'a'.repeat(3500) + '\n\n' + 'b'.repeat(6498);
            const chunks = chunkContent(content);
            expect(chunks.length).toBeGreaterThan(1);
            expect(chunks[0]).toBe('a'.repeat(3500) + '\n\n');
        });

        it('should split at line breaks when no paragraph break in window', () => {
            // \n at position 3500, no \n\n in window
            const content = 'a'.repeat(3500) + '\n' + 'b'.repeat(6499);
            const chunks = chunkContent(content);
            expect(chunks.length).toBeGreaterThan(1);
            expect(chunks[0]).toBe('a'.repeat(3500) + '\n');
        });

        it('should split at sentence boundary when no line breaks in window', () => {
            const content = 'a'.repeat(3500) + '. ' + 'b'.repeat(6498);
            const chunks = chunkContent(content);
            expect(chunks.length).toBeGreaterThan(1);
            expect(chunks[0]).toBe('a'.repeat(3500) + '. ');
        });

        it('should split at word boundary when no sentence boundary in window', () => {
            const content = 'a'.repeat(3500) + ' ' + 'b'.repeat(6499);
            const chunks = chunkContent(content);
            expect(chunks.length).toBeGreaterThan(1);
            expect(chunks[0]).toBe('a'.repeat(3500) + ' ');
        });

        it('should apply overlap between chunks', () => {
            // With paragraph break at 3500, first chunk ends at 3502
            // Second chunk starts at 3502 - 200 = 3302
            const content = 'a'.repeat(3500) + '\n\n' + 'b'.repeat(6498);
            const chunks = chunkContent(content);
            const firstEnd = chunks[0].length; // 3502
            expect(firstEnd).toBe(3502);
            // Second chunk starts at 3302, so it overlaps with first chunk
            const overlapRegion = chunks[0].slice(-200);
            expect(chunks[1].startsWith(overlapRegion)).toBe(true);
        });
    });

    describe('sanitizeText', () => {
        const sanitize = (text: string) => (service as any).sanitizeText(text);

        it('should remove lone high surrogates', () => {
            expect(sanitize('hello\uD800world')).toBe('helloworld');
        });

        it('should remove lone low surrogates', () => {
            expect(sanitize('hello\uDC00world')).toBe('helloworld');
        });

        it('should preserve valid surrogate pairs (emoji)', () => {
            expect(sanitize('hello ðŸ˜€ world')).toBe('hello ðŸ˜€ world');
        });

        it('should pass through normal text unchanged', () => {
            expect(sanitize('hello world')).toBe('hello world');
        });
    });

    describe('flattenMetadata', () => {
        const flatten = (meta: Record<string, unknown>) => (service as any).flattenMetadata(meta);

        it('should pass through string values', () => {
            expect(flatten({ title: 'Test' })).toEqual({ title: 'Test' });
        });

        it('should pass through number values', () => {
            expect(flatten({ count: 42 })).toEqual({ count: 42 });
        });

        it('should pass through boolean values', () => {
            expect(flatten({ active: true })).toEqual({ active: true });
        });

        it('should skip null and undefined values', () => {
            expect(flatten({ a: null, b: undefined, c: 'keep' })).toEqual({ c: 'keep' });
        });

        it('should JSON-stringify array values', () => {
            expect(flatten({ tags: ['a', 'b'] })).toEqual({ tags: '["a","b"]' });
        });

        it('should JSON-stringify object values', () => {
            expect(flatten({ nested: { x: 1 } })).toEqual({ nested: '{"x":1}' });
        });

        it('should create numeric timestamp for createdAt string', () => {
            const result = flatten({ createdAt: '2024-01-15T10:00:00Z' });
            expect(result.createdAt).toBe('2024-01-15T10:00:00Z');
            expect(result.createdAtTs).toBe(new Date('2024-01-15T10:00:00Z').getTime());
        });

        it('should create numeric timestamp for updatedAt string', () => {
            const result = flatten({ updatedAt: '2024-06-20T14:30:00Z' });
            expect(result.updatedAt).toBe('2024-06-20T14:30:00Z');
            expect(result.updatedAtTs).toBe(new Date('2024-06-20T14:30:00Z').getTime());
        });

        it('should not create timestamp for invalid date strings', () => {
            const result = flatten({ createdAt: 'not-a-date' });
            expect(result.createdAt).toBe('not-a-date');
            expect(result.createdAtTs).toBeUndefined();
        });

        it('should sanitize surrogate characters in string values', () => {
            const result = flatten({ title: 'test\uD800value' });
            expect(result.title).toBe('testvalue');
        });
    });

    describe('contentHash', () => {
        const hash = (text: string) => (service as any).contentHash(text);

        it('should return deterministic SHA256 hash truncated to 16 chars', () => {
            const expected = createHash('sha256').update('hello').digest('hex').slice(0, 16);
            expect(hash('hello')).toBe(expected);
            expect(hash('hello')).toHaveLength(16);
        });

        it('should produce same hash for same input', () => {
            expect(hash('test')).toBe(hash('test'));
        });

        it('should produce different hashes for different input', () => {
            expect(hash('abc')).not.toBe(hash('xyz'));
        });
    });

    // â”€â”€â”€ Scoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('computeKeywordScore', () => {
        const score = (content: string, terms: string[]) =>
            (service as any).computeKeywordScore(content, terms);

        it('should return 0 when no terms match', () => {
            expect(score('hello world', ['missing'])).toBe(0);
        });

        it('should compute score for single term with single occurrence (docLength=2000)', () => {
            // content length = 2000, term "fox" appears once
            const content = 'a'.repeat(1996) + ' fox';
            const result = score(content, ['fox']);
            // coverage = 1/1 = 1, normalizedTF = min(1, (1+log(1))/1/3) = 1/3
            // lengthFactor = 1/(1+log(2000/2000)) = 1
            // score = 1*0.6 + (1/3)*0.3 + 1*0.1 = 0.6 + 0.1 + 0.1 = 0.8
            expect(result).toBeCloseTo(0.8, 4);
        });

        it('should compute score for multiple terms all matching', () => {
            const content = 'a'.repeat(1988) + ' foo bar foo';
            const result = score(content, ['foo', 'bar']);
            // "foo" appears 2x: tf = 1 + log(2) = 1.693
            // "bar" appears 1x: tf = 1 + log(1) = 1
            // matchedTerms = 2, tfSum = 2.693
            // coverage = 2/2 = 1
            // normalizedTF = min(1, 2.693/2/3) = 0.4489
            // lengthFactor = 1/(1+log(2000/2000)) = 1
            // score = 1*0.6 + 0.4489*0.3 + 1*0.1 = 0.8347
            expect(result).toBeCloseTo(0.8347, 3);
        });

        it('should compute partial coverage when only some terms match', () => {
            const content = 'a'.repeat(1994) + ' hello';
            const result = score(content, ['hello', 'world']);
            // matchedTerms = 1, coverage = 1/2 = 0.5
            // normalizedTF = min(1, 1/1/3) = 0.333
            // lengthFactor = 1
            // score = 0.5*0.6 + 0.333*0.3 + 1*0.1 = 0.3 + 0.1 + 0.1 = 0.5
            expect(result).toBeCloseTo(0.5, 3);
        });

        it('should apply TF diminishing returns for repeated terms', () => {
            // "abc" repeated many times in 2000-char doc
            const content = 'abc '.repeat(500); // 2000 chars, "abc" appears 500 times
            const result = score(content, ['abc']);
            // count = 500, tf = 1 + log(500) = 1 + 6.215 = 7.215
            // normalizedTF = min(1, 7.215/1/3) = min(1, 2.405) = 1
            // coverage = 1, lengthFactor = 1/(1+log(2000/2000)) = 1
            // score = 0.6 + 1*0.3 + 1*0.1 = 1.0
            expect(result).toBeCloseTo(1.0, 3);
        });

        it('should apply length normalization penalty for long documents', () => {
            const shortContent = 'a'.repeat(1996) + ' fox'; // 2000 chars
            const longContent = 'a'.repeat(7996) + ' fox'; // 8000 chars
            const shortScore = score(shortContent, ['fox']);
            const longScore = score(longContent, ['fox']);
            // Longer doc gets lower length factor
            expect(longScore).toBeLessThan(shortScore);
        });
    });

    describe('applyRelevancyBoosts', () => {
        const applyBoosts = (results: SearchResult[], query: string) =>
            (service as any).applyRelevancyBoosts(results, query);

        it('should blend connector relevance_score into result score', () => {
            const results: SearchResult[] = [{
                id: '1', source: 'jira', content: '', score: 0.8,
                metadata: { relevance_score: 0.7 },
            }];
            applyBoosts(results, 'query');
            // relevanceBoost = 0.85 + 0.7 * 0.35 = 1.095
            // score = min(1, 0.8 * 1.095 * 1.0) = 0.876
            expect(results[0].score).toBeCloseTo(0.876, 3);
        });

        it('should apply 1.3x boost for exact title match', () => {
            const results: SearchResult[] = [{
                id: '1', source: 'jira', content: '', score: 0.5,
                metadata: { title: 'authentication issue' },
            }];
            applyBoosts(results, 'authentication issue');
            // boost = 1.3, score = min(1, 0.5 * 1.3) = 0.65
            expect(results[0].score).toBeCloseTo(0.65, 3);
        });

        it('should apply partial title match boost based on term ratio', () => {
            const results: SearchResult[] = [{
                id: '1', source: 'jira', content: '', score: 0.5,
                metadata: { title: 'login page' },
            }];
            applyBoosts(results, 'login form page');
            // queryTerms: ['login', 'form', 'page'], title has 'login' and 'page' â†’ 2/3
            // boost = 1 + (2/3) * 0.2 = 1.1333
            // score = min(1, 0.5 * 1.1333) = 0.5667
            expect(results[0].score).toBeCloseTo(0.5667, 3);
        });

        it('should also check subject field for title match', () => {
            const results: SearchResult[] = [{
                id: '1', source: 'gmail', content: '', score: 0.5,
                metadata: { subject: 'meeting notes' },
            }];
            applyBoosts(results, 'meeting notes');
            // Exact match via subject â†’ boost = 1.3
            expect(results[0].score).toBeCloseTo(0.65, 3);
        });

        it('should apply recency boost using source-specific half-life', () => {
            const now = Date.now();
            jest.spyOn(Date, 'now').mockReturnValue(now);

            // updatedAt = exactly 7 days ago (slack half-life = 7)
            const sevenDaysAgo = new Date(now - 7 * 24 * 60 * 60 * 1000).toISOString();
            const results: SearchResult[] = [{
                id: '1', source: 'slack', content: '', score: 0.5,
                metadata: { updatedAt: sevenDaysAgo },
            }];
            applyBoosts(results, 'query');
            // daysSince = 7, halfLife = 7, recencyScore = 0.5^1 = 0.5
            // boost = 1 + 0.5 * 0.08 = 1.04
            // score = min(1, 0.5 * 1.04) = 0.52
            expect(results[0].score).toBeCloseTo(0.52, 2);

            jest.restoreAllMocks();
        });

        it('should give maximum recency boost for very recent documents', () => {
            const now = Date.now();
            jest.spyOn(Date, 'now').mockReturnValue(now);

            const justNow = new Date(now).toISOString();
            const results: SearchResult[] = [{
                id: '1', source: 'jira', content: '', score: 0.5,
                metadata: { updatedAt: justNow },
            }];
            applyBoosts(results, 'query');
            // daysSince â‰ˆ 0, recencyScore â‰ˆ 1, boost â‰ˆ 1.08
            // score â‰ˆ 0.5 * 1.08 = 0.54
            expect(results[0].score).toBeCloseTo(0.54, 2);

            jest.restoreAllMocks();
        });

        it('should cap final score at 1.0', () => {
            const results: SearchResult[] = [{
                id: '1', source: 'jira', content: '', score: 0.95,
                metadata: { relevance_score: 0.85, title: 'exact match query' },
            }];
            applyBoosts(results, 'exact match query');
            expect(results[0].score).toBeLessThanOrEqual(1.0);
        });
    });

    // â”€â”€â”€ Where clause builders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('buildWhereClause', () => {
        const build = (where?: Record<string, unknown>, startDate?: string, endDate?: string) =>
            (service as any).buildWhereClause(where, startDate, endDate);

        it('should return undefined when no filters', () => {
            expect(build()).toBeUndefined();
            expect(build(undefined, undefined, undefined)).toBeUndefined();
        });

        it('should build clause from where object with string/number/boolean values', () => {
            expect(build({ status: 'open' })).toEqual({ status: 'open' });
            expect(build({ count: 5 })).toEqual({ count: 5 });
            expect(build({ active: true })).toEqual({ active: true });
        });

        it('should ignore non-primitive where values', () => {
            expect(build({ nested: { x: 1 }, valid: 'yes' })).toEqual({ valid: 'yes' });
        });

        it('should build startDate filter with $gte on createdAtTs', () => {
            const result = build(undefined, '2024-01-01');
            const ts = new Date('2024-01-01').getTime();
            expect(result).toEqual({ createdAtTs: { $gte: ts } });
        });

        it('should build endDate filter with end-of-day $lte on createdAtTs', () => {
            const result = build(undefined, undefined, '2024-01-31');
            const ts = new Date('2024-01-31T23:59:59.999Z').getTime();
            expect(result).toEqual({ createdAtTs: { $lte: ts } });
        });

        it('should combine multiple conditions with $and', () => {
            const result = build({ source: 'jira' }, '2024-01-01', '2024-12-31');
            expect(result).toEqual({
                $and: [
                    { source: 'jira' },
                    { createdAtTs: { $gte: new Date('2024-01-01').getTime() } },
                    { createdAtTs: { $lte: new Date('2024-12-31T23:59:59.999Z').getTime() } },
                ],
            });
        });
    });

    // â”€â”€â”€ Source-specific helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('getRecencyHalfLife', () => {
        const halfLife = (source: DataSource) => (service as any).getRecencyHalfLife(source);

        it.each([
            ['slack', 7], ['calendar', 14], ['gmail', 14],
            ['jira', 30], ['github', 60], ['confluence', 90], ['drive', 90],
        ] as [DataSource, number][])('should return %i days for %s', (source, expected) => {
            expect(halfLife(source)).toBe(expected);
        });
    });

    describe('getTimestampField', () => {
        const field = (source: DataSource) => (service as any).getTimestampField(source);

        it.each([
            ['slack', 'timestamp'], ['gmail', 'date'], ['calendar', 'start'],
            ['jira', 'updatedAt'], ['drive', 'updatedAt'], ['confluence', 'updatedAt'],
            ['github', 'updatedAt'],
        ] as [DataSource, string][])('should return "%s" for %s', (source, expected) => {
            expect(field(source)).toBe(expected);
        });
    });

    describe('getContextType', () => {
        const ctxType = (source: DataSource, meta: Record<string, unknown>) =>
            (service as any).getContextType(source, meta);

        it('should return "thread" for slack with threadTs', () => {
            expect(ctxType('slack', { threadTs: '123' })).toBe('thread');
        });

        it('should return "channel" for slack without threadTs', () => {
            expect(ctxType('slack', {})).toBe('channel');
        });

        it('should return "thread" for gmail', () => {
            expect(ctxType('gmail', {})).toBe('thread');
        });

        it('should return "issue" for jira comment', () => {
            expect(ctxType('jira', { type: 'comment' })).toBe('issue');
        });

        it('should return "project" for jira issue', () => {
            expect(ctxType('jira', { type: 'issue' })).toBe('project');
        });

        it('should return "folder" for drive', () => {
            expect(ctxType('drive', {})).toBe('folder');
        });

        it('should return "page" for confluence comment', () => {
            expect(ctxType('confluence', { type: 'comment' })).toBe('page');
        });

        it('should return "space" for confluence page', () => {
            expect(ctxType('confluence', { type: 'page' })).toBe('space');
        });

        it('should return "calendar" for calendar', () => {
            expect(ctxType('calendar', {})).toBe('calendar');
        });

        it('should return "pull_request" for github pr_comment', () => {
            expect(ctxType('github', { type: 'pr_comment' })).toBe('pull_request');
        });

        it('should return "pull_request" for github pr_review', () => {
            expect(ctxType('github', { type: 'pr_review' })).toBe('pull_request');
        });

        it('should return "repository" for github issue', () => {
            expect(ctxType('github', { type: 'issue' })).toBe('repository');
        });
    });

    describe('resolveParentDocumentId', () => {
        const resolve = (source: DataSource, meta: Record<string, unknown>) =>
            (service as any).resolveParentDocumentId(source, meta);

        it('should prefix confluence comment parentId with "confluence_"', () => {
            expect(resolve('confluence', { parentId: '12345', type: 'comment' })).toBe('confluence_12345');
        });

        it('should return raw parentId for non-confluence sources', () => {
            expect(resolve('jira', { parentId: 'PROJ-1' })).toBe('PROJ-1');
            expect(resolve('slack', { parentId: 'slack_C01_123' })).toBe('slack_C01_123');
        });

        it('should check parentDocId if parentId is missing', () => {
            expect(resolve('jira', { parentDocId: 'chunk-parent' })).toBe('chunk-parent');
        });

        it('should return null when no parent identifiers exist', () => {
            expect(resolve('jira', {})).toBeNull();
            expect(resolve('gmail', {})).toBeNull();
        });

        it('should return null for drive with path (no parent doc for folders)', () => {
            expect(resolve('drive', { path: '/folder/subfolder/file.txt' })).toBeNull();
        });
    });

    describe('buildDatapointWhereClause', () => {
        const build = (source: DataSource, meta: Record<string, unknown>) =>
            (service as any).buildDatapointWhereClause(source, meta);

        it('should use threadTs for slack threads', () => {
            expect(build('slack', { threadTs: '123.456' })).toEqual({ threadTs: '123.456' });
        });

        it('should fall back to channelId for slack', () => {
            expect(build('slack', { channelId: 'C01' })).toEqual({ channelId: 'C01' });
        });

        it('should return null for slack with no thread/channel', () => {
            expect(build('slack', {})).toBeNull();
        });

        it('should use threadId for gmail', () => {
            expect(build('gmail', { threadId: 'thread1' })).toEqual({ threadId: 'thread1' });
        });

        it('should use parentId for jira, fallback to project', () => {
            expect(build('jira', { parentId: 'PROJ-1' })).toEqual({ parentId: 'PROJ-1' });
            expect(build('jira', { project: 'PROJ' })).toEqual({ project: 'PROJ' });
            expect(build('jira', {})).toBeNull();
        });

        it('should use folderPath for drive', () => {
            expect(build('drive', { folderPath: '/docs' })).toEqual({ folderPath: '/docs' });
        });

        it('should extract folder from path for drive fallback', () => {
            expect(build('drive', { path: '/folder/file.txt' })).toEqual({ folderPath: '/folder' });
        });

        it('should use parentId for confluence, fallback to space', () => {
            expect(build('confluence', { parentId: '123' })).toEqual({ parentId: '123' });
            expect(build('confluence', { space: 'ENG' })).toEqual({ space: 'ENG' });
        });

        it('should return { source: "calendar" } for calendar', () => {
            expect(build('calendar', {})).toEqual({ source: 'calendar' });
        });

        it('should use parentId for github, fallback to repo', () => {
            expect(build('github', { parentId: 'pr-1' })).toEqual({ parentId: 'pr-1' });
            expect(build('github', { repo: 'org/repo' })).toEqual({ repo: 'org/repo' });
            expect(build('github', {})).toBeNull();
        });
    });

    describe('buildContextWhereClause', () => {
        const build = (source: DataSource, meta: Record<string, unknown>) =>
            (service as any).buildContextWhereClause(source, meta);

        it('should use channelId for slack', () => {
            expect(build('slack', { channelId: 'C01' })).toEqual({ channelId: 'C01' });
        });

        it('should use threadId for gmail', () => {
            expect(build('gmail', { threadId: 't1' })).toEqual({ threadId: 't1' });
        });

        it('should use project for jira', () => {
            expect(build('jira', { project: 'PROJ' })).toEqual({ project: 'PROJ' });
        });

        it('should use folderPath for drive', () => {
            expect(build('drive', { folderPath: '/docs' })).toEqual({ folderPath: '/docs' });
        });

        it('should extract folder from path for drive fallback', () => {
            expect(build('drive', { path: '/folder/file.txt' })).toEqual({ folderPath: '/folder' });
        });

        it('should use space for confluence', () => {
            expect(build('confluence', { space: 'ENG' })).toEqual({ space: 'ENG' });
        });

        it('should use repo for github', () => {
            expect(build('github', { repo: 'org/repo' })).toEqual({ repo: 'org/repo' });
        });

        it('should return null when no matching metadata exists', () => {
            expect(build('slack', {})).toBeNull();
            expect(build('gmail', {})).toBeNull();
            expect(build('jira', {})).toBeNull();
            expect(build('drive', {})).toBeNull();
            expect(build('confluence', {})).toBeNull();
            expect(build('github', {})).toBeNull();
        });
    });

    // â”€â”€â”€ upsertDocuments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('upsertDocuments', () => {
        it('should be a no-op for empty documents array', async () => {
            await service.upsertDocuments('jira', []);
            expect(mockClient.getOrCreateCollection).not.toHaveBeenCalled();
        });

        it('should upsert a single-chunk document with flattened metadata and content hash', async () => {
            const doc = {
                id: 'jira-1',
                source: 'jira' as DataSource,
                content: 'Short issue content',
                metadata: { title: 'Bug', createdAt: '2024-01-15T10:00:00Z', source: 'jira' },
            } as any;

            await service.upsertDocuments('jira', [doc]);

            expect(mockCollection.upsert).toHaveBeenCalledWith({
                ids: ['jira-1'],
                documents: ['Short issue content'],
                metadatas: [expect.objectContaining({
                    title: 'Bug',
                    createdAt: '2024-01-15T10:00:00Z',
                    createdAtTs: new Date('2024-01-15T10:00:00Z').getTime(),
                    source: 'jira',
                    _contentHash: contentHash('Short issue content'),
                })],
            });
        });

        it('should create chunk items for content exceeding MAX_CONTENT_LENGTH', async () => {
            const longContent = 'a'.repeat(9000);
            const doc = {
                id: 'doc-long',
                source: 'drive' as DataSource,
                content: longContent,
                metadata: { title: 'Long doc', source: 'drive' },
            } as any;

            await service.upsertDocuments('drive', [doc]);

            // Should have multiple chunks
            const upsertCall = mockCollection.upsert.mock.calls[0][0];
            expect(upsertCall.ids.length).toBeGreaterThan(1);
            expect(upsertCall.ids[0]).toBe('doc-long_chunk_0');
            expect(upsertCall.ids[1]).toBe('doc-long_chunk_1');
            // Check chunk metadata
            expect(upsertCall.metadatas[0]).toMatchObject({
                chunkIndex: 0,
                parentDocId: 'doc-long',
            });
            expect(upsertCall.metadatas[0].totalChunks).toBeGreaterThan(1);
        });

        it('should route unchanged content to metadata-only update', async () => {
            const content = 'Existing content';
            const hash = contentHash(content);

            // Mock existing hash that matches
            mockCollection.get.mockResolvedValue({
                ids: ['doc1'],
                metadatas: [{ _contentHash: hash }],
            });

            const doc = {
                id: 'doc1',
                source: 'jira' as DataSource,
                content,
                metadata: { title: 'Updated title', source: 'jira' },
            } as any;

            await service.upsertDocuments('jira', [doc]);

            // Should call update (metadata-only) instead of upsert
            expect(mockCollection.update).toHaveBeenCalledWith({
                ids: ['doc1'],
                metadatas: [expect.objectContaining({
                    title: 'Updated title',
                    _contentHash: hash,
                })],
            });
            // Should NOT call upsert (no content changes)
            expect(mockCollection.upsert).not.toHaveBeenCalled();
        });

        it('should route changed content to full upsert', async () => {
            // Mock existing hash that does NOT match
            mockCollection.get.mockResolvedValue({
                ids: ['doc1'],
                metadatas: [{ _contentHash: 'old-hash-different' }],
            });

            const doc = {
                id: 'doc1',
                source: 'jira' as DataSource,
                content: 'New content',
                metadata: { title: 'Doc', source: 'jira' },
            } as any;

            await service.upsertDocuments('jira', [doc]);

            expect(mockCollection.upsert).toHaveBeenCalledWith({
                ids: ['doc1'],
                documents: ['New content'],
                metadatas: [expect.objectContaining({
                    _contentHash: contentHash('New content'),
                })],
            });
        });

        it('should handle pre-chunked documents', async () => {
            const doc = {
                id: 'github_file_1',
                source: 'github' as DataSource,
                content: 'Full file content',
                metadata: { title: 'app.ts', source: 'github' },
                preChunked: { chunks: ['chunk one', 'chunk two'] },
            } as any;

            await service.upsertDocuments('github', [doc]);

            const upsertCall = mockCollection.upsert.mock.calls[0][0];
            expect(upsertCall.ids).toEqual(['github_file_1_chunk_0', 'github_file_1_chunk_1']);
            expect(upsertCall.documents).toEqual(['chunk one', 'chunk two']);
            expect(upsertCall.metadatas[0]).toMatchObject({
                chunkIndex: 0,
                totalChunks: 2,
                parentDocId: 'github_file_1',
            });
        });
    });

    // â”€â”€â”€ Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('search', () => {
        beforeEach(() => {
            (service as any).embeddingFunction = {
                generate: jest.fn().mockResolvedValue([[0.1, 0.2, 0.3]]),
            };
        });

        it('should convert cosine distances to similarity scores in vector search', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [['id1', 'id2']],
                documents: [['content1', 'content2']],
                metadatas: [[{}, {}]],
                distances: [[0.2, 0.5]],
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
                limit: 10,
            });

            // distance 0.2 â†’ score = max(0, 1 - 0.2) = 0.8
            // distance 0.5 â†’ score = max(0, 1 - 0.5) = 0.5
            expect(result.results).toHaveLength(2);
            expect(result.results[0].score).toBeCloseTo(0.8, 1);
            expect(result.results[1].score).toBeCloseTo(0.5, 1);
            expect(result.results[0].source).toBe('jira');
        });

        it('should use pre-computed embedding for vector search', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [[]], documents: [[]], metadatas: [[]], distances: [[]],
            });

            await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
            });

            // Should use queryEmbeddings, not queryTexts
            expect(mockCollection.query).toHaveBeenCalledWith(
                expect.objectContaining({
                    queryEmbeddings: [[0.1, 0.2, 0.3]],
                }),
            );
            expect(mockCollection.query).toHaveBeenCalledWith(
                expect.not.objectContaining({ queryTexts: expect.anything() }),
            );
        });

        it('should perform keyword search with $contains filter', async () => {
            mockCollection.get.mockResolvedValue({
                ids: ['id1'],
                documents: ['a'.repeat(1994) + ' hello'],
                metadatas: [{}],
            });

            const result = await service.search('hello', {
                sources: ['jira' as DataSource],
                searchType: 'keyword',
                limit: 10,
            });

            // Verify whereDocument filter
            expect(mockCollection.get).toHaveBeenCalledWith(
                expect.objectContaining({
                    whereDocument: { $contains: 'hello' },
                }),
            );
            expect(result.results.length).toBeGreaterThan(0);
        });

        it('should use $and for multi-word keyword search', async () => {
            mockCollection.get.mockResolvedValue({
                ids: [], documents: [], metadatas: [],
            });

            await service.search('hello world', {
                sources: ['jira' as DataSource],
                searchType: 'keyword',
            });

            expect(mockCollection.get).toHaveBeenCalledWith(
                expect.objectContaining({
                    whereDocument: {
                        $and: [{ $contains: 'hello' }, { $contains: 'world' }],
                    },
                }),
            );
        });

        it('should deduplicate chunks keeping highest-scoring per parent', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [['standalone', 'parent1_chunk_0', 'parent1_chunk_1']],
                documents: [['doc A', 'chunk 0', 'chunk 1']],
                metadatas: [[
                    {},
                    { parentDocId: 'parent1', chunkIndex: 0, totalChunks: 2 },
                    { parentDocId: 'parent1', chunkIndex: 1, totalChunks: 2 },
                ]],
                distances: [[0.2, 0.3, 0.4]], // scores: 0.8, 0.7, 0.6
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
                limit: 10,
            });

            // Should have 2 results: standalone + best chunk of parent1
            expect(result.results).toHaveLength(2);
            expect(result.total).toBe(2);
            // Standalone (0.8) should be first
            expect(result.results[0].id).toBe('standalone');
            // Best chunk (0.7) gets multi-chunk boost: 0.7 * (1 + min(log(2)*0.05, 0.15))
            expect(result.results[1].id).toBe('parent1_chunk_0');
        });

        it('should apply multi-chunk boost for documents with multiple matching chunks', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [['p_chunk_0', 'p_chunk_1', 'p_chunk_2']],
                documents: [['c0', 'c1', 'c2']],
                metadatas: [[
                    { parentDocId: 'p' },
                    { parentDocId: 'p' },
                    { parentDocId: 'p' },
                ]],
                distances: [[0.2, 0.3, 0.4]], // scores: 0.8, 0.7, 0.6
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
                limit: 10,
            });

            // Best chunk score = 0.8, chunkCount = 3
            // Multi-chunk boost: 0.8 * (1 + min(log(3)*0.05, 0.15))
            // log(3) â‰ˆ 1.0986, * 0.05 = 0.0549
            // boosted = 0.8 * 1.0549 â‰ˆ 0.8439
            expect(result.results).toHaveLength(1);
            expect(result.results[0].score).toBeCloseTo(0.8439, 2);
        });

        it('should apply offset and limit pagination', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [['a', 'b', 'c']],
                documents: [['da', 'db', 'dc']],
                metadatas: [[{}, {}, {}]],
                distances: [[0.1, 0.3, 0.5]], // scores: 0.9, 0.7, 0.5
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
                limit: 1,
                offset: 1,
            });

            // After sort: a(0.9), b(0.7), c(0.5)
            // offset=1, limit=1 â†’ [b]
            expect(result.results).toHaveLength(1);
            expect(result.results[0].id).toBe('b');
            expect(result.total).toBe(3);
        });

        it('should pass where clause to vector search', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [[]], documents: [[]], metadatas: [[]], distances: [[]],
            });

            await service.search('test', {
                sources: ['jira' as DataSource],
                searchType: 'vector',
                where: { project: 'PROJ' },
                startDate: '2024-01-01',
            });

            expect(mockCollection.query).toHaveBeenCalledWith(
                expect.objectContaining({
                    where: {
                        $and: [
                            { project: 'PROJ' },
                            { createdAtTs: { $gte: new Date('2024-01-01').getTime() } },
                        ],
                    },
                }),
            );
        });

        it('should query multiple sources in parallel', async () => {
            mockCollection.query.mockResolvedValue({
                ids: [['id1']], documents: [['doc1']], metadatas: [[{}]], distances: [[0.3]],
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource, 'slack' as DataSource],
                searchType: 'vector',
                limit: 20,
            });

            // Each source gets its own query
            expect(mockClient.getOrCreateCollection).toHaveBeenCalledTimes(2);
            // Results from both sources are merged
            expect(result.results.length).toBeGreaterThanOrEqual(1);
        });

        it('should handle search failure for a source gracefully', async () => {
            // First source succeeds, second fails at collection level (caught by try-catch)
            mockClient.getOrCreateCollection
                .mockResolvedValueOnce(mockCollection)
                .mockRejectedValueOnce(new Error('Connection failed'));

            mockCollection.query.mockResolvedValue({
                ids: [['id1']], documents: [['doc1']], metadatas: [[{}]], distances: [[0.3]],
            });

            const result = await service.search('test', {
                sources: ['jira' as DataSource, 'slack' as DataSource],
                searchType: 'vector',
            });

            // Should still return results from the successful source
            expect(result.results).toHaveLength(1);
            expect(result.results[0].id).toBe('id1');
        });
    });

    // â”€â”€â”€ Navigate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('navigate', () => {
        it('should return null current when document is not found', async () => {
            // Default mock returns empty results for all getDocument calls
            const result = await service.navigate('nonexistent', 'next', 'chunk');

            expect(result.current).toBeNull();
            expect(result.related).toEqual([]);
            expect(result.navigation).toEqual({
                hasPrev: false,
                hasNext: false,
                parentId: null,
                contextType: 'unknown',
            });
        });

        it('should navigate to previous chunk', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'doc1_chunk_1') {
                    return Promise.resolve({
                        ids: ['doc1_chunk_1'],
                        documents: ['Chunk 1 content'],
                        metadatas: [{ chunkIndex: 1, totalChunks: 3, parentDocId: 'doc1' }],
                    });
                }
                if (args.ids?.[0] === 'doc1_chunk_0') {
                    return Promise.resolve({
                        ids: ['doc1_chunk_0'],
                        documents: ['Chunk 0 content'],
                        metadatas: [{ chunkIndex: 0, totalChunks: 3, parentDocId: 'doc1' }],
                    });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            const result = await service.navigate('doc1_chunk_1', 'prev', 'chunk');

            expect(result.current).not.toBeNull();
            expect(result.current!.id).toBe('doc1_chunk_1');
            expect(result.related).toHaveLength(1);
            expect(result.related[0].id).toBe('doc1_chunk_0');
        });

        it('should navigate to next chunk', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'doc1_chunk_0') {
                    return Promise.resolve({
                        ids: ['doc1_chunk_0'],
                        documents: ['Chunk 0 content'],
                        metadatas: [{ chunkIndex: 0, totalChunks: 2, parentDocId: 'doc1' }],
                    });
                }
                if (args.ids?.[0] === 'doc1_chunk_1') {
                    return Promise.resolve({
                        ids: ['doc1_chunk_1'],
                        documents: ['Chunk 1 content'],
                        metadatas: [{ chunkIndex: 1, totalChunks: 2, parentDocId: 'doc1' }],
                    });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            const result = await service.navigate('doc1_chunk_0', 'next', 'chunk');

            expect(result.related).toHaveLength(1);
            expect(result.related[0].id).toBe('doc1_chunk_1');
            expect(result.navigation.hasNext).toBe(true);
        });

        it('should return empty related when at first chunk and navigating prev', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'doc1_chunk_0') {
                    return Promise.resolve({
                        ids: ['doc1_chunk_0'],
                        documents: ['content'],
                        metadatas: [{ chunkIndex: 0, totalChunks: 2, parentDocId: 'doc1' }],
                    });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            const result = await service.navigate('doc1_chunk_0', 'prev', 'chunk');
            expect(result.related).toEqual([]);
        });

        it('should navigate to parent document', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'child-doc') {
                    return Promise.resolve({
                        ids: ['child-doc'],
                        documents: ['child content'],
                        metadatas: [{ parentId: 'parent-doc' }],
                    });
                }
                if (args.ids?.[0] === 'parent-doc') {
                    return Promise.resolve({
                        ids: ['parent-doc'],
                        documents: ['parent content'],
                        metadatas: [{ title: 'Parent' }],
                    });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            const result = await service.navigate('child-doc', 'parent', 'datapoint');

            expect(result.related).toHaveLength(1);
            expect(result.related[0].id).toBe('parent-doc');
            expect(result.navigation.parentId).toBe('parent-doc');
        });

        it('should navigate to children documents', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'parent-doc') {
                    return Promise.resolve({
                        ids: ['parent-doc'],
                        documents: ['parent content'],
                        metadatas: [{ id: 'logical-id' }],
                    });
                }
                if (args.where?.parentId === 'logical-id') {
                    return Promise.resolve({
                        ids: ['child-1', 'child-2'],
                        documents: ['c1', 'c2'],
                        metadatas: [{ parentId: 'logical-id' }, { parentId: 'logical-id' }],
                    });
                }
                if (args.where?.parentDocId === 'parent-doc') {
                    return Promise.resolve({ ids: [], documents: [], metadatas: [] });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            const result = await service.navigate('parent-doc', 'children', 'datapoint');

            expect(result.related.length).toBeGreaterThanOrEqual(2);
        });

        it('should set contextType correctly based on source and metadata', async () => {
            mockCollection.get.mockImplementation((args: any) => {
                if (args.ids?.[0] === 'slack-msg') {
                    return Promise.resolve({
                        ids: ['slack-msg'],
                        documents: ['message content'],
                        metadatas: [{ threadTs: '123.456', channelId: 'C01' }],
                    });
                }
                return Promise.resolve({ ids: [], documents: [], metadatas: [] });
            });

            // Since all sources use the same mock, 'slack-msg' will be found in 'jira' (first source)
            // To properly test, we'd need per-source collections. Let's just verify the method works.
            const result = await service.navigate('slack-msg', 'next', 'datapoint');
            expect(result.current).not.toBeNull();
            // contextType comes from whichever source finds it first
            expect(result.navigation.contextType).toBeDefined();
        });
    });

    // â”€â”€â”€ Other public methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    describe('deleteDocument', () => {
        it('should delete document and its chunks', async () => {
            mockCollection.get.mockResolvedValue({
                ids: ['doc1_chunk_0', 'doc1_chunk_1'],
            });

            await service.deleteDocument('jira', 'doc1');

            expect(mockCollection.delete).toHaveBeenCalledWith({ ids: ['doc1'] });
            expect(mockCollection.delete).toHaveBeenCalledWith({ ids: ['doc1_chunk_0', 'doc1_chunk_1'] });
        });

        it('should handle missing document gracefully', async () => {
            mockCollection.delete.mockRejectedValueOnce(new Error('Not found'));
            mockCollection.get.mockResolvedValue({ ids: [] });

            await expect(service.deleteDocument('jira', 'nonexistent')).resolves.not.toThrow();
        });
    });

    describe('getDocument', () => {
        it('should return document when found', async () => {
            mockCollection.get.mockResolvedValue({
                ids: ['doc1'],
                documents: ['content'],
                metadatas: [{ title: 'Test' }],
            });

            const result = await service.getDocument('jira', 'doc1');
            expect(result).toEqual({
                id: 'doc1',
                source: 'jira',
                content: 'content',
                metadata: { title: 'Test' },
                score: 1,
            });
        });

        it('should return null when document not found', async () => {
            mockCollection.get.mockResolvedValue({ ids: [] });
            const result = await service.getDocument('jira', 'missing');
            expect(result).toBeNull();
        });
    });

    describe('deleteCollection', () => {
        it('should delete collection from client and clear cache', async () => {
            // Pre-populate cache
            (service as any).collections.set('jira', mockCollection);

            await service.deleteCollection('jira');

            expect(mockClient.deleteCollection).toHaveBeenCalledWith({ name: 'collector_jira' });
            expect((service as any).collections.has('jira')).toBe(false);
        });
    });
});
